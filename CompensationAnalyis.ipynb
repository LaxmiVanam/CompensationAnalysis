{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Author: Laxmi Vanam**\n",
    "- **Email: laxmivanam05@gmail.com **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script pulls in several features and the target salary data, builds and tests several predictive models to predict the salary on unseen data using the best model. This uses the concept of Object oriented programming and is built using the 4 pillars.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc\n",
    "\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "\n",
    "#modeling\n",
    "from sklearn.model_selection import cross_validate, cross_val_score,GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defines Data class to create train and test dataframes from the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataprep:    \n",
    "    \n",
    "    def __init__ (self, train_feature_file, train_target_file, test_feature_file,  \n",
    "                    target, id_col):\n",
    "        '''To initialize instance variables passed through object'''\n",
    "        \n",
    "        self.id_col = id_col\n",
    "        self.target = target\n",
    "        self.train_df = self._create_train_dataframe(train_feature_file,train_target_file)      \n",
    "        self.test_df = self._create_test_dataframe(test_feature_file)\n",
    "        \n",
    "   \n",
    "    \n",
    "    def _create_train_dataframe(self,train_feature_file,train_target_file,validate_train_files = True,clean_dataset = True\n",
    "                                ):\n",
    "        '''Private method to create train dataframe by preprocessing and label encoding categorical columns'''\n",
    "        self.train_feature_df = self._load_data(train_feature_file)\n",
    "        self.train_target_df = self._load_data(train_target_file)    \n",
    "        \n",
    "        if validate_train_files:\n",
    "            self.validate_train_records( self.train_feature_df, self.train_target_df,self.id_col)\n",
    "            train_df = self._merge_dfs(self.train_feature_df,self.train_target_df,self.id_col)    \n",
    "            \n",
    "        if clean_dataset:\n",
    "            train_df = self._cleandataframe(train_df,id_col,target)\n",
    "            train_df = self._shuffledataframe(train_df)\n",
    "            \n",
    "             \n",
    "        return train_df    \n",
    "    \n",
    "    def _create_test_dataframe(self,test_file ):\n",
    "        '''Private method to create test dataframe by label encoding categorical columns'''\n",
    "     \n",
    "        test_df = self._load_data(test_file)   \n",
    "        \n",
    "        #if label_encode:            \n",
    "            #self._label_encode_check(test_df,self.category_cols)            \n",
    "        return test_df    \n",
    "    \n",
    "    def print_dataframe_shape(self, df, df_name):\n",
    "        '''To print the shape of the train/ test dataset'''\n",
    "        print('The shape of the %s dataset is %s' %(df_name, df.shape))     \n",
    "           \n",
    "    \n",
    "    def _load_data(self,file):\n",
    "        '''Private method to load input files to pandas dataframes'''\n",
    "        return pd.read_csv(file)\n",
    "    \n",
    "    def validate_train_records(self,train_feature_df, train_target_df, id_col):  \n",
    "        if ((train_feature_df[id_col].nunique() != len(train_feature_df)) \n",
    "            | (train_target_df[id_col].nunique() != len(train_target_df))):\n",
    "            print ('Duplicate ID records exist in the dataframes')\n",
    "        print('The number of records in features file that are not in target file are',len(set(train_feature_df[id_col])) - len(set(train_target_df[id_col])))\n",
    "        print('The number of records in target file that are not in features file are',len(set(train_target_df[id_col])) - len(set(train_feature_df[id_col])))\n",
    "        \n",
    "    \n",
    "    def _merge_dfs(self,dataframe1,dataframe2, key , how = 'inner',left_index = False, right_index = False):\n",
    "        '''Private method to merge train features and the target column for train dataframe'''\n",
    "        return pd.merge(dataframe1,dataframe2,on = key)#.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "   \n",
    "    def _cleandataframe(self,df,id_col, target): \n",
    "        '''Private method to drop duplicates and also the records with no salary from training dataframe'''\n",
    "        df = df.drop_duplicates(subset = id_col)\n",
    "        df = df[df[target] >0]\n",
    "        return df\n",
    "    \n",
    "    def _shuffledataframe(self,df): \n",
    "        '''Private method to shuffle train dataset for modeling'''\n",
    "        df = shuffle(df).reset_index(drop = True)\n",
    "        return df    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines input parameters for the Dataprep class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input files path\n",
    "input_file_path = 'C:/Users/laxmi/CompAnalysis/rawdata/'\n",
    "\n",
    "#Define input files\n",
    "train_feature_file = input_file_path + 'train_features.csv'\n",
    "train_target_file =  input_file_path + 'train_salaries.csv'\n",
    "test_feature_file =  input_file_path + 'test_features.csv'\n",
    "\n",
    "#Define other parameters for Data class\n",
    "category_cols = ['companyId','jobType','degree','major','industry']\n",
    "numeric_cols = ['yearsExperience','milesFromMetropolis']\n",
    "target = 'salary'\n",
    "id_col = 'jobId'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating Dataprep class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in features file that are not in target file are 0\n",
      "The number of records in target file that are not in features file are 0\n"
     ]
    }
   ],
   "source": [
    "data = Dataprep(train_feature_file,train_target_file,test_feature_file, target, id_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the train dataset is (999995, 9)\n",
      "The shape of the test dataset is (1000000, 8)\n"
     ]
    }
   ],
   "source": [
    "data.print_dataframe_shape(data.train_df, 'train')\n",
    "data.print_dataframe_shape(data.test_df, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines This is for encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingCatCols:\n",
    "    ''''''\n",
    "    def __init__ (self,df_train, df_test, category_cols):\n",
    "            self.df_train = df_train\n",
    "            self.df_test= df_test\n",
    "            self.category_cols = list(category_cols)        \n",
    "            self.existing_label_encoded_cols = {}\n",
    "                   \n",
    "    def label_encode_check(self,df,cols):\n",
    "        '''Private method to check for existence of label encoders for categorical columns and uses it. Otherwise, it creates new label encoders'''\n",
    "        for col in cols:                    \n",
    "            if col in self.existing_label_encoded_cols:\n",
    "                self._encode_now(df,col,self.existing_label_encoded_cols[col].all())                 \n",
    "            else:\n",
    "                self._encode_now(df,col)     \n",
    "        return self.encoded_df        \n",
    "                \n",
    "    def inverse_encode_check(self,cols):\n",
    "        '''Method to inverse label encoded values to the original values'''\n",
    "        for col in cols:                    \n",
    "            if col in self.existing_label_encoded_cols:\n",
    "                self._inverselabel_encode_now(df,col)                 \n",
    "            else:\n",
    "                raise ValueError(\"Label incoders must be defined before calling inverse function\")    \n",
    "                \n",
    "                                  \n",
    "    def _inverse_encode_now(self,col):\n",
    "        '''Private method to create label encoders for the given categorical column and adds them to the dictionary'''    \n",
    "        le = self.existing_label_encoded_cols[col]\n",
    "        df[col]  = le.inverse_transform(df[col])       \n",
    "                                \n",
    "    def _encode_now(self,df,col, le = None):\n",
    "        '''Private method to create label encoders for the given categorical column and adds them to the dictionary'''    \n",
    "        if le:     \n",
    "            df[col] = le.transform(df[col])\n",
    "        else:                \n",
    "            le = LabelEncoder()\n",
    "            le.fit(df[col])\n",
    "            df[col] = le.transform(df[col])\n",
    "            self.existing_label_encoded_cols[col] = df[col] \n",
    "            self.encoded_df =  pd.DataFrame(self.existing_label_encoded_cols)\n",
    "            \n",
    "    def concat_encodedcat_numericfields(self, df1, df2) :\n",
    "        return pd.concat([df1,df2], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating Preprocessing class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PreprocessingCatCols = PreprocessingCatCols( data.train_df,data.test_df,  category_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding train values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_df = PreprocessingCatCols.label_encode_check(data.train_df,   PreprocessingCatCols.category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_df = PreprocessingCatCols.label_encode_check(data.test_df,   PreprocessingCatCols.category_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = PreprocessingCatCols.concat_encodedcat_numericfields(encoded_train_df,data.train_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df = PreprocessingCatCols.concat_encodedcat_numericfields(encoded_test_df,data.train_df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define label/ parameters for FeatureEngineering and ModelContainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label for turning feature engineering on/off\n",
    "Addinggroupstats_label = True\n",
    "\n",
    "#Parameters for ModelContainer class\n",
    "num_procs = 4   #Number of processes for parallel runs\n",
    "verbose_lvl = 0 #Verbose level for modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining FeatureEngineering class to add additional fetures for modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Addinggroupstats:\n",
    "    \n",
    "    def __init__ (self,data, cols_to_filter = None):\n",
    "        '''To initialize instance variSEz2ables passed through object'''\n",
    "        self.data = data\n",
    "        self.target_col = 'salary'\n",
    "        self.cols_to_filter = ['yearsExperience','milesFromMetropolis']\n",
    "        self.categoies_for_grouping = ['companyId','jobType','degree','major','industry']\n",
    "        self.groups = data.train_df.groupby(self.categoies_for_grouping)\n",
    "        self.train_features = data.train_df.iloc[:,2:-1]\n",
    "    \n",
    "    def add_group_statistics(self):        \n",
    "        '''To add group statistics for each of the categorical columns'''\n",
    "        self.group_stats_df = pd.DataFrame({})\n",
    "        self.group_stats_df = self._get_group_statistics()  \n",
    "        self.merged_data = self._merge_derived_columns_to_original(final_train_df, self.group_stats_df, self.categoies_for_grouping)\n",
    "        return self.merged_data\n",
    "    \n",
    "    def  _get_group_statistics(self):\n",
    "        '''To calculate various statistics of target salary'''\n",
    "        target_col = self.data.target\n",
    "        \n",
    "        group_stats_df = pd.DataFrame({'group_mean': self.groups[target_col].mean()}) \n",
    "        group_stats_df['group_max'] = self.groups[target_col].max()\n",
    "        group_stats_df['group_min'] = self.groups[target_col].min()\n",
    "        group_stats_df['group_std'] = self.groups[target_col].std()\n",
    "        group_stats_df['group_median'] = self.groups[target_col].median()\n",
    "        group_stats_df.reset_index(inplace = True)\n",
    "        \n",
    "        return group_stats_df\n",
    "    \n",
    "    def _merge_derived_columns_to_original(self,df1, df2, keys , how = 'left',fillna = False):\n",
    "        '''To merge the statistics to the original columns in train data frame'''\n",
    "        merged_df = pd.merge(df1, df2, on=keys)\n",
    "        merged_df.fillna(0, inplace = True)\n",
    "        return merged_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating Addinggroupstats class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Addinggroupstats_label:\n",
    "    feature_generator = Addinggroupstats(data)\n",
    "    feature_generator.add_group_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defines ModelContainer class to explore and analyse various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelContainer:\n",
    "   \n",
    "    def __init__(self,modellist = []):\n",
    "        ''' To initialize instance variables passed through object'''   \n",
    "        self.best_model = None\n",
    "        self.predictions = None\n",
    "        self.modellist = modellist\n",
    "        self.scores = {} \n",
    "                \n",
    "    def add_model(self, modelname):\n",
    "        '''To add a new model to the list'''\n",
    "        self.modellist.append(modelname)\n",
    "        \n",
    "    def cross_validate(self,  features,target,  k = 3, num_procs = num_procs):\n",
    "        '''To cross validate models using given data, k value and the number of processes'''\n",
    "        self.trainfeatures = features#data.train_df.iloc[:,2:-1]\n",
    "        self.traintarget = target#data.train_df.iloc[:,-1]\n",
    "   \n",
    "        for model in self.modellist:\n",
    "            score = cross_val_score(model,self.trainfeatures, self.traintarget,cv = k\n",
    "                                                 ,scoring=('neg_mean_squared_error')\n",
    "                                                )     \n",
    "            self.scores[model] =  -1*np.mean(score)\n",
    "           \n",
    "    def select_best_model(self):\n",
    "        '''To choose the best model of all the given ones by the score'''\n",
    "        self.best_model = min(self.scores, key = self.scores.get)\n",
    "        return self.best_model\n",
    "    \n",
    "        \n",
    "    def best_model_fit(self,traifeatures,traintarget):\n",
    "        '''To fit the best model to the train dataset'''  \n",
    "        self.best_model.fit(traifeatures, traintarget)\n",
    "        \n",
    "    def best_model_predict(self,testfeatures):\n",
    "        '''To predict the target value for the test dataset'''\n",
    "        self.predictions = self.best_model.predict(testfeatures)    \n",
    "        \n",
    "    def save_results(self):\n",
    "        ''' To save the test results if needed'''\n",
    "        pass\n",
    "    \n",
    "    @staticmethod     # use as decorator   \n",
    "    def get_feature_importance(model, cols):\n",
    "        '''Static method which can be accessed outside the class with no self/ cls parameter as well'''\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            feature_importance = model.feature_importances_\n",
    "            feature_imp_df = pd.DataFrame({'Features':cols,'Importance' : feature_importance})         \n",
    "            feature_imp_df.sort_values(by = 'Importance', ascending = False,inplace = True )\n",
    "            feature_imp_df.set_index('Features', inplace=True, drop=True)\n",
    "            feature_imp_df.plot.bar()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Feature Importance does not exist for the current model\")\n",
    "        \n",
    "    def print_summary(self):\n",
    "        '''To print summary of the final model'''\n",
    "        print('\\nModel Summary:\\n')\n",
    "        self.get_feature_importance (self.best_model,data.train_df.iloc[:,2:-1].columns) \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating Modelcontainer class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcontainer = ModelContainer()\n",
    "modelcontainer.add_model(LinearRegression())\n",
    "modelcontainer.add_model(Ridge(alpha = 1.0))\n",
    "\n",
    "modelcontainer.add_model(RandomForestRegressor(n_estimators = 60,n_jobs = num_procs, max_depth = 15, min_samples_split = 80, max_features = 8,  verbose = verbose_lvl))\n",
    "\n",
    "modelcontainer.add_model(GradientBoostingRegressor(n_estimators = 40, max_depth = 7, loss = 'ls',   verbose = verbose_lvl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False): 1499.1246097719795,\n",
       " Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "       normalize=False, random_state=None, solver='auto', tol=0.001): 1499.1246097032472,\n",
       " RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
       "                       max_depth=15, max_features=8, max_leaf_nodes=None,\n",
       "                       max_samples=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=80, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=60, n_jobs=4, oob_score=False,\n",
       "                       random_state=None, verbose=0, warm_start=False): 1500.9937395431825,\n",
       " GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
       "                           init=None, learning_rate=0.1, loss='ls', max_depth=7,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=40,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0, warm_start=False): 1502.0315916624236}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelcontainer.cross_validate(feature_generator.merged_data,data.train_df.iloc[:,-1], k=2)\n",
    "modelcontainer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "      normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelcontainer.select_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcontainer.best_model_fit(data.train_df.iloc[:,1:-1],data.train_df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcontainer.best_model_predict(data.test_df.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary:\n",
      "\n",
      "Feature Importance does not exist for the current model\n"
     ]
    }
   ],
   "source": [
    "modelcontainer.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CompensationPredictor.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'CompensationPredictor.csv'\n",
    "joblib.dump(modelcontainer.best_model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This joblib can be loaded further on future data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
